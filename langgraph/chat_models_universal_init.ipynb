{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48eb093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-anthropic langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Don't forget to set your environment variables for the API keys of the respective providers!\n",
    "# For example, you can set them in your terminal or in a .env file:\n",
    "# export OPENAI_API_KEY=\"your_openai_api_key\"\n",
    "\n",
    "# Returns a langchain_openai.ChatOpenAI instance.\n",
    "gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0)\n",
    "# Returns a langchain_anthropic.ChatAnthropic instance.\n",
    "claude_opus = init_chat_model(\"claude-3-opus-20240229\", model_provider=\"anthropic\", temperature=0)\n",
    "# Returns a langchain_google_vertexai.ChatVertexAI instance.\n",
    "gemini_15 = init_chat_model(\"gemini-2.5-pro\", model_provider=\"google_genai\", temperature=0)\n",
    "\n",
    "# Since all model integrations implement the ChatModel interface, you can use them in the same way.\n",
    "print(\"GPT-4o: \" + gpt_4o.invoke(\"what's your name\").content + \"\\n\")\n",
    "print(\"Claude Opus: \" + claude_opus.invoke(\"what's your name\").content + \"\\n\")\n",
    "print(\"Gemini 2.5: \" + gemini_15.invoke(\"what's your name\").content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a534f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o = init_chat_model(\"gpt-4o\", temperature=0)\n",
    "claude_opus = init_chat_model(\"claude-3-opus-20240229\", temperature=0)\n",
    "gemini_15 = init_chat_model(\"gemini-2.5-pro\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable_model = init_chat_model(temperature=0)\n",
    "\n",
    "configurable_model.invoke(\"what's your name\", config={\"configurable\": {\"model\": \"gpt-4o\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb87522",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurable_model.invoke(\n",
    "    \"what's your name\", config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acc0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_llm = init_chat_model(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
    "    config_prefix=\"first\",  # useful when you have a chain with multiple models\n",
    ")\n",
    "\n",
    "first_llm.invoke(\"what's your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04945b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_llm.invoke(\n",
    "    \"what's your name\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"first_model\": \"claude-3-5-sonnet-20240620\",\n",
    "            \"first_temperature\": 0.5,\n",
    "            \"first_max_tokens\": 100,\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585bac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "class GetPopulation(BaseModel):\n",
    "    \"\"\"Get the current population in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "llm = init_chat_model(temperature=0)\n",
    "llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
    "\n",
    "llm_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
    ").tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\",\n",
    "    config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}},\n",
    ").tool_calls"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
